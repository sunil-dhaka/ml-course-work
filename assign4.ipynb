{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5:40 = 550 Sec\n",
    "- 2:40 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TOC:`\n",
    "* [Question-1](#Question-1)\n",
    "  * [Part-(a)](#Part-(a))\n",
    "  * [Part-(b)](#Part-(b))\n",
    "  * [Part-(c)](#Part-(c))\n",
    "  * [Part-(d)](#Part-(d))\n",
    "* [Question-2](#Question-2)\n",
    "  * [Part-(a)](#Part-(a))\n",
    "  * [Part-(b)](#Part-(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use any additional libraries you want, but if you don't explicitly code stuff that I have explicitly asked you to code, you will not get marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Kernel k-means\n",
    "\n",
    "Let's try and end our course on a happy note. The smiley dataset you see below is made up of many clusters of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import numpy.random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def getFigure( sizex = 7, sizey = 7 ):\n",
    "    fig = plt.figure( figsize = (sizex, sizey) )\n",
    "    return fig\n",
    "\n",
    "def plot2D( X, fig, color = 'r', marker = '+', size = 100, empty = False ):\n",
    "    plt.figure( fig.number )\n",
    "    if empty:\n",
    "        plt.scatter( X[:,0], X[:,1], s = size, facecolors = 'none', edgecolors = color, marker = marker  )\n",
    "    else:\n",
    "        plt.scatter( X[:,0], X[:,1], s = size, c = color, marker = marker )\n",
    "\n",
    "\n",
    "def genCrescentData( d, n, mu, r, flipped = False ):\n",
    "    X = np.vstack( (np.cos( np.linspace( 0, np.pi, n ) ), np.sin( np.linspace( 0, np.pi, n ) ) ) ).T\n",
    "    if flipped:\n",
    "        X[:,1] = -np.abs( X[:,1] )\n",
    "    else:\n",
    "        X[:,1] = np.abs( X[:,1] )\n",
    "    X = (X * r) + mu\n",
    "    return X\n",
    "\n",
    "def genSphericalData( d, n, mu, r ):\n",
    "    X = rnd.normal( 0, 1, (n, d) )\n",
    "    norms = lin.norm( X, axis = 1 )\n",
    "    X = X / norms[:, np.newaxis]\n",
    "    X = (X * r) + mu\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "n = 200\n",
    "\n",
    "mu1 = np.array( [0,0] )\n",
    "mu2 = np.array( [0,1] )\n",
    "mu3 = np.array( [0,0] )\n",
    "mu4 = np.array( [-3,5] )\n",
    "mu5 = np.array( [3,5] )\n",
    "\n",
    "tmp1 = genCrescentData( d, n, mu1, 1 )\n",
    "tmp2 = genCrescentData( d, n, mu2, 5, flipped = True )\n",
    "tmp3 = genSphericalData( d, n, mu3, 10 )\n",
    "tmp4 = genSphericalData( d, n, mu4, 1 )\n",
    "tmp5 = genSphericalData( d, n, mu5, 1 )\n",
    "X = np.vstack( (tmp1, tmp2, tmp3, tmp4, tmp5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = getFigure( 8, 8 )\n",
    "plot2D( X, fig, size = 50, color = 'b', marker = 'o' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Can you implement the k-means algorithm to cluster this dataset? Visualize your output. [10 points] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using sklearn(K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KMeans(n_clusters=4,verbose=False,init='k-means++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=(model.fit_predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "func to plot 2D data with cluster IDs\n",
    "Input:\n",
    "data_2d: Data points in 2D :: If higher dim then can give PCA(2) applied data\n",
    "labels: cluster IDs array\n",
    "n_cluster: no of clusters; default = 4\n",
    "Output:\n",
    "plots a 2D scatter map with legend\n",
    "'''\n",
    "def plot_clusters(data_2d,labels,n_cluster=4):\n",
    "    # filter rows of original data\n",
    "    # for label in labels:\n",
    "    fig = plt.figure( figsize = (8, 8) )\n",
    "    plt.figure(fig.number)\n",
    "    for cluster in range(n_cluster):\n",
    "        label0 = data_2d[labels == cluster]\n",
    "        plt.scatter(label0[:,0] , label0[:,1],label=cluster)\n",
    "    plt.legend(fontsize='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get max float point from system\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]])\n",
    "np.delete(a,[0,1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((X-X[0,])**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.delete(X,[0,1,999],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=np.sum(X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k menas clustering algo implementation\n",
    "Input:\n",
    "X: data points\n",
    "n_cluster: no of clusters; default = 4\n",
    "init_style: initial cluster choosing style; default = k-menas\n",
    "    - k-means: to choose initial clusters randomly\n",
    "    - k-menas++: to choose initial clusters usefully\n",
    "Output:\n",
    "Cluster labels for data points\n",
    "''' \n",
    "def get_KMeans_clusters(X,n_cluster=4,init_style='k-means',n_iter=100):\n",
    "\n",
    "    # to reproduce results\n",
    "    # np.random.seed(1)\n",
    "\n",
    "    n_points=X.shape[0]\n",
    "    n_dim=X.shape[1]\n",
    "\n",
    "    '''\n",
    "    Get Inital Centroids\n",
    "    '''\n",
    "    # cluster centers are chosen to be K of the data points themselves #\n",
    "    if init_style=='k-means':\n",
    "\n",
    "        # this method simply chooses random n_cluster points from permuatated index\n",
    "        init_centroids_index=rnd.permutation(n_points)[:n_cluster]\n",
    "        init_centroids=X[init_centroids_index]\n",
    "        # another way is to create k-many random centroids that are not data points\n",
    "        #TODO: implement this one also with if-else cond\n",
    "    elif init_style=='k-means++':\n",
    "        # in this we choose centroids that are more representative of the sample points\n",
    "        # empty array and list\n",
    "        init_centroids=np.empty((n_cluster, n_dim))\n",
    "        init_centroids_index=list()\n",
    "\n",
    "        init_centroids_index.append(rnd.randint(n_points))\n",
    "        init_centroids[0]=X[init_centroids_index[-1]]\n",
    "       \n",
    "        for i in range(1,n_cluster):\n",
    "            \n",
    "            # new_X=np.delete(X,init_centroids_index,axis=0)\n",
    "            # no need to create a new array(2D) from X based on only unselected points\n",
    "            # bcz probability for them would be zero; so they would not be selected again\n",
    "             \n",
    "            tmp=np.empty((n_points,i))\n",
    "            for j in range(i):\n",
    "                tmp[:,j]=np.sum(np.sqrt((X-init_centroids[j,:])**2),axis=1)\n",
    "\n",
    "            tmp_min=np.min(tmp,axis=1) # min(D(X)) for each unselected point\n",
    "\n",
    "            # convert them into probabilities\n",
    "            tmp_prob=tmp_min/np.sum(tmp_min)\n",
    "            # print(np.min(tmp_prob))\n",
    "\n",
    "            # possible index values\n",
    "            values=list(range(0,1000))\n",
    "            # choose an index randomly based on probability\n",
    "            next_centriod= np.random.choice(a=values, size=1, p=tmp_prob)[0]\n",
    "            # add next centroid to the list and empty array\n",
    "            init_centroids_index.append(next_centriod)\n",
    "            init_centroids[i]=X[init_centroids_index[-1]]\n",
    "    else:\n",
    "        print('give a valid choice for init_style')\n",
    "        return None\n",
    "    \n",
    "    '''\n",
    "    KMeans Iterations\n",
    "    '''\n",
    "    curr_centroids=init_centroids\n",
    "\n",
    "    # to store cluster assignment and error info after each iteration\n",
    "    cluster_history=list()\n",
    "    wacc_history=list()\n",
    "\n",
    "    # for stopping/convergance criterian\n",
    "    error_tol=10**(-4)\n",
    "    iter_error_diff=1 \n",
    "    iter_count=0\n",
    "    new_objective_value=sys.float_info.max\n",
    "\n",
    "    while (iter_error_diff>error_tol) and (iter_count<n_iter):\n",
    "        # increase iter_count\n",
    "        iter_count+=1\n",
    "        \n",
    "        # create empty temp array to store euclidean dist-sum from cluster centroids\n",
    "        # do it for each cluster centroid\n",
    "        tmp_array=np.empty((n_points, n_cluster))\n",
    "\n",
    "        for cluster in range(n_cluster):\n",
    "            # here distance is euclidean\n",
    "            tmp_array[:,cluster]=np.sum((X-curr_centroids[cluster,:])**2,axis=1)\n",
    "        \n",
    "        # assign cluster to each point based on min distance from the cluster\n",
    "        tmp_cluster_assign=np.argmin(tmp_array,axis=1)\n",
    "        cluster_history.append(tmp_cluster_assign)\n",
    "        \n",
    "        # update centeroids by cluster point mean\n",
    "        curr_objective_value=0\n",
    "        for cluster in range(n_cluster):\n",
    "            tmp_cluster_points=X[(tmp_cluster_assign==cluster),:] # 2D array\n",
    "            tmp_centroid=np.mean(tmp_cluster_points,axis=0) # 1D mean array\n",
    "            curr_objective_value+=np.sum(np.sum((tmp_cluster_points-tmp_centroid)**2,axis=0),axis=0) # scalar\n",
    "            curr_centroids[cluster,:]=tmp_centroid # update cluster centroid\n",
    "        wacc_history.append(curr_objective_value) # add curr error value to history\n",
    "\n",
    "        # update difference\n",
    "        iter_error_diff=new_objective_value-curr_objective_value\n",
    "        new_objective_value=curr_objective_value\n",
    "        \n",
    "    # return cluster ids for each of the data points in X with some history info\n",
    "    return {'final':tmp_cluster_assign,'history':cluster_history,'iter-count':iter_count,'wacc-history':wacc_history,'final-centroids':curr_centroids,'start-centroid':init_centroids_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_clusters=5\n",
    "model=get_KMeans_clusters(X,n_cluster=no_of_clusters)\n",
    "plot_clusters(X,model['final'],n_cluster=no_of_clusters)\n",
    "# centroids=model['final-centroids']\n",
    "# plt.scatter(centroids[:,0],centroids[:,1],s=300,c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_KMeans_clusters(X,n_cluster=no_of_clusters,init_style='k-means++')\n",
    "\n",
    "plot_clusters(X,model['final'],n_cluster=no_of_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implement k-means++ to initialize cluster centers usefully. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What value of k gives you the best clustering? Are you happy with the quality of the clustering? [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elbow analysis with kmeans random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting multiple k-means algorithms with different n_clusters and storing the values in an empty list\n",
    "objective_value = []\n",
    "max_cluster=15\n",
    "for cluster in range(1,max_cluster+1):\n",
    "    kmeans = get_KMeans_clusters(X,n_cluster=cluster)\n",
    "    objective_value.append(kmeans['wacc-history'][-1])\n",
    "\n",
    "# plotting the results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(list(range(1,max_cluster+1)), objective_value, marker='o')\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('objective-value')\n",
    "plt.title('K-means Random')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- around 5-6 clusters there seems an elbow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elbow analysis with kmeans++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting multiple k-means algorithms with different n_clusters and storing the values in an empty list\n",
    "objective_value = []\n",
    "max_cluster=15\n",
    "for cluster in range(1,max_cluster+1):\n",
    "    kmeans = get_KMeans_clusters(X,n_cluster=cluster,init_style='k-means++') # with kmeans++\n",
    "    objective_value.append(kmeans['wacc-history'][-1])\n",
    "\n",
    "# plotting the results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(list(range(1,max_cluster+1)), objective_value, marker='o')\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('objective-value')\n",
    "plt.title('K-means++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- around 5-6 clusters there seems an elbow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AM I HAPPY WITH THIS CLUSTERING?\n",
    "- NO, I DON'T THINK SO.\n",
    "- There are only linear cluster coundaries. I was hoping it would cluster `eye1`, `eye1`, `nose`, `mouth` and `face` in different clusters but that would need non-linear boundaries at least to cluster face into one cluster we will need non-linear boundary. Other items that could have been clustered with linear boundaries, that also is not being done by it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) I'm going to say that we want to be able to do better than this. So I want you to kernelize your k-means algorithm with a Gaussian kernel. Visualize the clustering output of your kernel k-means algorithm [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "# set this variable to change lambda in guassian kernel\n",
    "LAMBDA=1\n",
    "def pair_kernel_dist(x,y):\n",
    "    return np.exp(-LAMBDA * np.sum((x-y)**2))\n",
    "    \n",
    "def pair_kernel_dist_total(x,cluster_points):\n",
    "    # math is bit involving given in markdowns\n",
    "    tic=time()\n",
    "    n_points_=len(cluster_points)\n",
    "    return_dist=n_points_**2\n",
    "    for i,point1 in enumerate(cluster_points):\n",
    "        for j,point2 in enumerate(cluster_points):\n",
    "            if i!=j:\n",
    "                return_dist+=pair_kernel_dist(point1,point2)-pair_kernel_dist(x,point1)-pair_kernel_dist(x,point2)\n",
    "    toc=time()\n",
    "    print(toc-tic)\n",
    "    return return_dist\n",
    "# def kernel_dist(X,centroid):\n",
    "#     tic=time()\n",
    "#     return_array=np.empty(X.shape[0])\n",
    "#     for i,x in enumerate(X):\n",
    "#         return_array[i]=pair_kernel_dist_total(x,centroid)\n",
    "#     toc=time()\n",
    "#     print(toc-tic)\n",
    "#     return return_array\n",
    "def kernel_dist(X,centroid):\n",
    "    # exact formula is given in kernel kmeans slide \n",
    "    return 2*(1-(np.exp(-LAMBDA * np.sum((X-centroid)**2,axis=1))))\n",
    "\n",
    "'''\n",
    "k menas clustering algo implementation\n",
    "Input:\n",
    "X: data points\n",
    "n_cluster: no of clusters; default = 4\n",
    "init_style: initial cluster choosing style; default = k-menas\n",
    "    - k-means: to choose initial clusters randomly\n",
    "    - k-menas++: to choose initial clusters usefully\n",
    "Output:\n",
    "Cluster labels for data points\n",
    "''' \n",
    "def get_kernel_KMeans_clusters(X,n_cluster=4,init_style='k-means',n_iter=10):\n",
    "\n",
    "    # to reproduce results\n",
    "    # np.random.seed(1)\n",
    "\n",
    "    n_points=X.shape[0]\n",
    "    n_dim=X.shape[1]\n",
    "    \n",
    "    '''\n",
    "    Get Inital Centroids\n",
    "    '''\n",
    "    # cluster centers are chosen to be K of the data points themselves #\n",
    "    if init_style=='k-means':\n",
    "\n",
    "        # this method simply chooses random n_cluster points from permuatated index\n",
    "        init_centroids_index=rnd.permutation(n_points)[:n_cluster]\n",
    "        init_centroids=X[init_centroids_index]\n",
    "        # another way is to create k-many random centroids that are not data points\n",
    "        #TODO: implement this one also with if-else cond\n",
    "    elif init_style=='k-means++':\n",
    "        # in this we choose centroids that are more representative of the sample points\n",
    "        # empty array and list\n",
    "        init_centroids=np.empty((n_cluster, n_dim))\n",
    "        init_centroids_index=list()\n",
    "\n",
    "        init_centroids_index.append(rnd.randint(n_points))\n",
    "        init_centroids[0]=X[init_centroids_index[-1]]\n",
    "       \n",
    "        for i in range(1,n_cluster):\n",
    "            \n",
    "            # new_X=np.delete(X,init_centroids_index,axis=0)\n",
    "            # no need to create a new array(2D) from X based on only unselected points\n",
    "            # bcz probability for them would be zero; so they would not be selected again\n",
    "             \n",
    "            tmp=np.empty((n_points,i))\n",
    "            for j in range(i):\n",
    "                #TODO:done\n",
    "                tmp[:,j]=np.sqrt(kernel_dist(X,init_centroids[j,:]))\n",
    "\n",
    "            tmp_min=np.min(tmp,axis=1) # min(D(X)) for each unselected point\n",
    "\n",
    "            # convert them into probabilities\n",
    "            tmp_prob=tmp_min/np.sum(tmp_min)\n",
    "            # print(np.min(tmp_prob))\n",
    "\n",
    "            # possible index values\n",
    "            values=list(range(0,1000))\n",
    "            # choose an index randomly based on probability\n",
    "            next_centriod= np.random.choice(a=values, size=1, p=tmp_prob)[0]\n",
    "            # add next centroid to the list and empty array\n",
    "            init_centroids_index.append(next_centriod)\n",
    "            init_centroids[i]=X[init_centroids_index[-1]]\n",
    "    else:\n",
    "        print('give a valid choice for init_style')\n",
    "        return None\n",
    "    \n",
    "    '''\n",
    "    KMeans Iterations\n",
    "    '''\n",
    "    # curr_centroids=list()\n",
    "    # for i in range(n_cluster):\n",
    "    #     curr_centroids.append(init_centroids[i,])\n",
    "    curr_centroids=init_centroids\n",
    "\n",
    "    # to store cluster assignment and error info after each iteration\n",
    "    cluster_history=list()\n",
    "    wacc_history=list()\n",
    "\n",
    "    # for stopping/convergance criterian\n",
    "    error_tol=10**(-4)\n",
    "    iter_error_diff=1 \n",
    "    iter_count=0\n",
    "    new_objective_value=sys.float_info.max\n",
    "\n",
    "    while (iter_error_diff>error_tol) and (iter_count<n_iter):\n",
    "        # increase iter_count\n",
    "        iter_count+=1\n",
    "        \n",
    "        # create empty temp array to store euclidean dist-sum from cluster centroids\n",
    "        # do it for each cluster centroid\n",
    "        tmp_array=np.empty((n_points, n_cluster))\n",
    "\n",
    "        for cluster in range(n_cluster):\n",
    "            # here distance is euclidean\n",
    "            #TODO:done\n",
    "            tmp_array[:,cluster]=kernel_dist(X,curr_centroids[cluster,:])\n",
    "        # print(tmp_cluster_assign)\n",
    "        # assign cluster to each point based on min distance from the cluster\n",
    "        tmp_cluster_assign=np.argmin(tmp_array,axis=1)\n",
    "        cluster_history.append(tmp_cluster_assign)\n",
    "        # print(tmp_cluster_assign)\n",
    "        # update centeroids by cluster point mean\n",
    "        curr_objective_value=0\n",
    "        for cluster in range(n_cluster):\n",
    "            tmp_cluster_points=X[(tmp_cluster_assign==cluster),:] # 2D array\n",
    "            #TODO:??\n",
    "            tmp_centroid=np.mean(tmp_cluster_points,axis=0) # 1D mean array\n",
    "            #TODO:done:done\n",
    "            # print(tmp_cluster_points)\n",
    "            curr_objective_value+=np.sum(kernel_dist(tmp_cluster_points,tmp_cluster_points)) # scalar\n",
    "            curr_centroids[cluster,]=tmp_centroid # update cluster centroid\n",
    "        # print(curr_centroids)\n",
    "        wacc_history.append(curr_objective_value) # add curr error value to history\n",
    "\n",
    "        # update difference\n",
    "        iter_error_diff=new_objective_value-curr_objective_value\n",
    "        new_objective_value=curr_objective_value\n",
    "    return tmp_cluster_assign\n",
    "    # return cluster ids for each of the data points in X with some history info\n",
    "    # return {'final':tmp_cluster_assign,'history':cluster_history,'iter-count':iter_count,}#'wacc-history':wacc_history,'final-centroids':curr_centroids,'start-centroid':init_centroids_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X,get_kernel_KMeans_clusters(X,n_iter=1000,n_cluster=5,init_style=\"k-means++\"),n_cluster=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the below one is painfully slow; Time complexity for each update iteration: $\\mathcal{O}(KN^3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "# set this variable to change lambda in guassian kernel\n",
    "LAMBDA=1\n",
    "def pair_kernel_dist(x,y):\n",
    "    return np.exp(-LAMBDA * np.sum((x-y)**2))\n",
    "    \n",
    "def pair_kernel_dist_total(x,cluster_points):\n",
    "    # math is bit involving given in markdowns\n",
    "    # tic=time()\n",
    "    n_points_=len(cluster_points)\n",
    "    return_dist=0\n",
    "    for i,point1 in enumerate(cluster_points):\n",
    "        return_dist-=(2/n_points_)*pair_kernel_dist(x,point1)\n",
    "        \n",
    "    # toc=time()\n",
    "    # print(toc-tic)\n",
    "    return return_dist\n",
    "def kernel_dist(X,centroid):\n",
    "    tic=time()\n",
    "    n_points_=len(centroid)\n",
    "    centroid_points_dist=n_points_\n",
    "    for i in range(n_points_):\n",
    "        for j in range(n_points_):\n",
    "            # this one is 2 times faster than the below one\n",
    "            if j>i:\n",
    "                centroid_points_dist+=2*pair_kernel_dist(centroid[i,],centroid[j,])\n",
    "            # below one is inefficient\n",
    "            # if i!=j:\n",
    "            #     centroid_points_dist+=pair_kernel_dist(centroid[i,],centroid[j,])\n",
    "    centroid_points_dist=centroid_points_dist/(n_points_**2)\n",
    "    toc=time()\n",
    "    print(toc-tic)\n",
    "    \n",
    "    return_array=np.empty(X.shape[0])\n",
    "    for i,x in enumerate(X):\n",
    "        return_array[i]=pair_kernel_dist_total(x,centroid)+centroid_points_dist\n",
    "    \n",
    "    return return_array\n",
    "def k(X,centroid):\n",
    "    # exact formula is given in kernel kmeans slide \n",
    "    return 2*(1-(np.exp(-LAMBDA * np.sum((X-centroid)**2,axis=1))))\n",
    "\n",
    "'''\n",
    "k menas clustering algo implementation\n",
    "Input:\n",
    "X: data points\n",
    "n_cluster: no of clusters; default = 4\n",
    "init_style: initial cluster choosing style; default = k-menas\n",
    "    - k-means: to choose initial clusters randomly\n",
    "    - k-menas++: to choose initial clusters usefully\n",
    "Output:\n",
    "Cluster labels for data points\n",
    "''' \n",
    "def get_kernel_KMeans_clusters(X,n_cluster=4,init_style='k-means',n_iter=2):\n",
    "\n",
    "    # to reproduce results\n",
    "    # np.random.seed(1)\n",
    "\n",
    "    n_points=X.shape[0]\n",
    "    n_dim=X.shape[1]\n",
    "\n",
    "    '''\n",
    "    Get Inital Centroids\n",
    "    '''\n",
    "    # cluster centers are chosen to be K of the data points themselves #\n",
    "    if init_style=='k-means':\n",
    "\n",
    "        # this method simply chooses random n_cluster points from permuatated index\n",
    "        init_centroids_index=rnd.permutation(n_points)[:n_cluster]\n",
    "        init_centroids=X[init_centroids_index]\n",
    "        # another way is to create k-many random centroids that are not data points\n",
    "        #TODO: implement this one also with if-else cond\n",
    "    elif init_style=='k-means++':\n",
    "        # in this we choose centroids that are more representative of the sample points\n",
    "        # empty array and list\n",
    "        init_centroids=np.empty((n_cluster, n_dim))\n",
    "        init_centroids_index=list()\n",
    "\n",
    "        init_centroids_index.append(rnd.randint(n_points))\n",
    "        init_centroids[0]=X[init_centroids_index[-1]]\n",
    "       \n",
    "        for i in range(1,n_cluster):\n",
    "            \n",
    "            # new_X=np.delete(X,init_centroids_index,axis=0)\n",
    "            # no need to create a new array(2D) from X based on only unselected points\n",
    "            # bcz probability for them would be zero; so they would not be selected again\n",
    "             \n",
    "            tmp=np.empty((n_points,i))\n",
    "            for j in range(i):\n",
    "                #TODO:done\n",
    "                # tmp[:,j]=np.sqrt(kernel_dist(X,init_centroids[j,:]))\n",
    "                tmp[:,j]=np.sqrt(k(X,init_centroids[j,:]))\n",
    "\n",
    "            tmp_min=np.min(tmp,axis=1) # min(D(X)) for each unselected point\n",
    "\n",
    "            # convert them into probabilities\n",
    "            tmp_prob=tmp_min/np.sum(tmp_min)\n",
    "            print(np.sum(tmp_min))\n",
    "\n",
    "            # possible index values\n",
    "            values=list(range(0,1000))\n",
    "            # choose an index randomly based on probability\n",
    "            next_centriod= np.random.choice(a=values, size=1, p=tmp_prob)[0]\n",
    "            # add next centroid to the list and empty array\n",
    "            init_centroids_index.append(next_centriod)\n",
    "            init_centroids[i]=X[init_centroids_index[-1]]\n",
    "    else:\n",
    "        print('give a valid choice for init_style')\n",
    "        return None\n",
    "    \n",
    "    '''\n",
    "    KMeans Iterations\n",
    "    '''\n",
    "    curr_centroids=list()\n",
    "    for i in range(n_cluster):\n",
    "        curr_centroids.append(init_centroids[i,])\n",
    "\n",
    "    # to store cluster assignment and error info after each iteration\n",
    "    cluster_history=list()\n",
    "    wacc_history=list()\n",
    "\n",
    "    # for stopping/convergance criterian\n",
    "    error_tol=10**(-4)\n",
    "    iter_error_diff=1 \n",
    "    iter_count=0\n",
    "    new_objective_value=sys.float_info.max\n",
    "\n",
    "    while iter_count<n_iter:\n",
    "        # increase iter_count\n",
    "        iter_count+=1\n",
    "        \n",
    "        # create empty temp array to store euclidean dist-sum from cluster centroids\n",
    "        # do it for each cluster centroid\n",
    "        tmp_array=np.empty((n_points, n_cluster))\n",
    "\n",
    "        for cluster in range(n_cluster):\n",
    "            # here distance is euclidean\n",
    "            #TODO:done\n",
    "            tmp_array[:,cluster]=kernel_dist(X,curr_centroids[cluster])\n",
    "        # print(tmp_cluster_assign)\n",
    "        # assign cluster to each point based on min distance from the cluster\n",
    "        tmp_cluster_assign=np.argmin(tmp_array,axis=1)\n",
    "        cluster_history.append(tmp_cluster_assign)\n",
    "        # print(tmp_cluster_assign)\n",
    "        # update centeroids by cluster point mean\n",
    "        curr_objective_value=0\n",
    "        for cluster in range(n_cluster):\n",
    "            tmp_cluster_points=X[(tmp_cluster_assign==cluster),:] # 2D array\n",
    "            #TODO:??\n",
    "            # tmp_centroid=np.mean(tmp_cluster_points,axis=0) # 1D mean array\n",
    "            #TODO:done:done\n",
    "            # print(tmp_cluster_points)\n",
    "            # curr_objective_value+=np.sum(kernel_dist(tmp_cluster_points,tmp_cluster_points)) # scalar\n",
    "            curr_centroids[cluster]=tmp_cluster_points # update cluster centroid\n",
    "        # print(curr_centroids)\n",
    "        wacc_history.append(curr_objective_value) # add curr error value to history\n",
    "\n",
    "        # update difference\n",
    "        # iter_error_diff=new_objective_value-curr_objective_value\n",
    "        # new_objective_value=curr_objective_value\n",
    "    return tmp_cluster_assign\n",
    "    # return cluster ids for each of the data points in X with some history info\n",
    "    # return {'final':tmp_cluster_assign,'history':cluster_history,'iter-count':iter_count,}#'wacc-history':wacc_history,'final-centroids':curr_centroids,'start-centroid':init_centroids_index}a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X,get_kernel_KMeans_clusters(X,n_cluster=5,n_iter=20,init_style='k-means++'),n_cluster=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it is too slow; have to use other techniques that might work faster; basically transfer data into hilbert space and use k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Expectation-Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how to use EM to learn parameters for Gaussian mixture models last week. Specifically, for a GMM described by the set of parameters $\\{\\pi_k, \\mu_k, \\Sigma_k \\}_{k=1}^K$, we saw that the E-step boils down to figuring out the expected assignment of clusters based on a responsibility judgment proportional to $\\pi_k~N(\\mu_k, \\Sigma_k)$, given curent parameter estimates, followed by using GDA MLE updates assuming the current expected assignment in the M-step to update parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Can you derive the E-step and M-step for an EM algorithm that would work for a Gaussian mixture model wherein the mixture weights $\\pi_k$ `are known`, and the covariances are restricted to be spherical, i.e. $\\Sigma_k = \\sigma^2_k I$? [20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM for Modified GMM\n",
    "The modification is that we know class probabilities of Gaussian mixture model. In stansard GMM, we estimate class probabilities($\\pi_k$) also by EM algorithm iterations.\\\n",
    "\n",
    "**Model Info:**\\\n",
    "Supppose $X_1,X_2,\\ldots,X_n \\sim F$, where $F$ is mixture of $K$ Gaussian distribution. And we know the weights of individual Gaussian in the mixture, it is known to be $\\pi_k$.\\\n",
    "We have seen that there is no analytical solution for the estimation of mean and covariance of Gaussians. We could solve it using gradient-based tools, that would only lead to more complication. Rather we introduce a latent variable and try to use latent variable model to solve for mean and covariance of Gaussians.\\\n",
    "\n",
    "**Parameters:**\\\n",
    "Since we already know class prob. we only need to get estimate for mean and covariance of each Gaussian in the mixture. So model parametrs are:\n",
    "$$\n",
    "\\Theta=\\{\\mu_k,\\sigma_k^2\\}_{k=1}^K\n",
    "$$\n",
    "\n",
    "**Complete log likelihood(CLL):**\\\n",
    "`Suppose` we have the information that $X_i$ is coming from which of the $K$ Gaussian populations. Thus, suppose the complete data was of the form:\n",
    "$$\n",
    "(X_1,Z_1),(X_2,Z_2),\\ldots,(X_n,Z_n)\n",
    "$$\n",
    "where each $Z_i = k$ means that $X_i$ is from population $k$. If this complete data is\n",
    "available to us, then the joint density for one observation is[using Bayes rule]:\n",
    "$$\n",
    "p(x_i,z_i|\\Theta)=p(x_i|z_i,\\Theta)p(z_i|\\Theta)\n",
    "$$\n",
    "For the full data complete likelihood can be given, assuming that observations are `i.i.d.`:\n",
    "$$\n",
    "\\mathcal{L}(\\Theta|X)=p(X,Z|\\Theta) \\\\\n",
    "=\\prod_{n=1}^N p(x_i,z_i|\\Theta) \\\\\n",
    "=\\prod_{n=1}^N p(x_i|z_i,\\Theta)p(z_i|\\Theta) \\\\\n",
    "=\\prod_{n=1}^N \\left(\\prod_{k=1}^K \\pi_k^{z_{nk}} \\prod_{k=1}^K p(x_i|z_i=k,\\Theta)^{z_{nk}}\\right)\\\\\n",
    "=\\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k p(x_i|z_i=k,\\Theta)]^{z_{nk}} \\\\\n",
    "=\\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k \\mathcal{N}(x_i|\\mu_k,\\sigma_k^2 I)]^{z_{nk}}\n",
    "$$\n",
    "Log of the complete likelihood can be given as:\n",
    "$$\n",
    "CLL=log(\\mathcal{L}(\\Theta|X)=p(X,Z|\\Theta)) \\\\\n",
    "= log \\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k \\mathcal{N}(x_i|\\mu_k,\\sigma_k^2 I)]^{z_{nk}} \\\\\n",
    "= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk}[log\\pi_k + log \\mathcal{N}(x_i|\\mu_k,\\sigma_k^2 I)]\n",
    "$$\n",
    "Now we have complete log likelihood in our hand. We can proceed to get EM steps for EM algo.\\\n",
    "\n",
    "**EM steps:**\\\n",
    "We will look at the general setup of $K$ groups, so that the density for $X_1,\\ldots,X_n$ is\n",
    "$$\n",
    "p(x|\\Theta)=\\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k,\\sigma_k^2 I)\n",
    "$$\n",
    "The setup is same. As we supposed complete data is $\\{(X_n,Z_n)\\}_{n=1}^N$. First, ee would need to get Expectation of CLL(E-step) for its maximization in M-step. In CLL we need condition distribution of $p(Z|X,\\Theta)$. Since observations are `i.i.d.` we can easily get seperatly for each n and k,\n",
    "$$\n",
    "p(z_n=k|x_n,\\Theta)=\\frac{p(x_n|z_n=k,\\Theta)p(z_n=k|\\Theta)}{p(x_n|\\Theta)} \\\\\n",
    "=\\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k,\\sigma_k^2 I)}{p(x_n|\\Theta)} \\\\\n",
    "=\\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k,\\sigma_k^2 I)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n|\\mu_j,\\sigma_j^2 I)}\n",
    "$$\n",
    "One can easily check that $\\sum_{k=1}^K p(z_n=k|x_n,\\Theta) = 1$. For notation ease, we denote it by $\\gamma_{nk}$. $\\gamma_{nk}$ are itself quantities of interest since they tell us the probability of the $i_{th}$ observation being in class $k$. This helps in classifying the observed data.\\\n",
    "\n",
    "**Expectation of CLL:**\\\n",
    "Note that here $\\hat{\\Theta}$ is the estimate of $\\Theta$ in previous EM iteration. I also have used standard properties of expectation here.\n",
    "$$\n",
    "\\mathbf{E}(CLL)=\\mathbf{E}_{p(Z|X,\\Theta)}[log(X,Z|\\Theta)] \\\\\n",
    "=\\mathbf{E}_{p(Z|X,\\Theta)}\\left[ \\sum_{n=1}^N \\sum_{k=1}^K z_{nk}[log\\pi_k + log \\mathcal{N}(x_i|\\mu_k,\\sigma_k^2 I)] \\right] \\\\\n",
    "=\\sum_{n=1}^N \\sum_{k=1}^K \\mathbf{E} \\left[ z_{nk}[log\\pi_k + log \\mathcal{N}(x_i|\\mu_k,\\sigma_k^2 I)] \\right] \\\\\n",
    "=\\sum_{n=1}^N \\sum_{k=1}^K \\left[ \\mathbf{E}[z_{nk}][log\\pi_k + log \\mathcal{N}(x_i|\\mu_k,\\sigma_k^2 I)] \\right] \\\\\n",
    "$$\n",
    "\n",
    "**Maximization of Expectation:**\\\n",
    "In M step to get updates for $\\Theta=\\{\\mu_k,\\sigma_k^2\\}_{k=1}^K$, we maximize the expecation like we do in MLE computation.\n",
    "$$\n",
    "\\hat{\\Theta}=argmax_{\\Theta} \\mathbf{E}(CLL) \\\\\n",
    "$$\n",
    "After substituting Gaussian distribution in CLL, we solve above optimization problem for each parameter,\n",
    "$$\n",
    "\\mathbf{E}(CLL)=\\sum_{n=1}^N \\sum_{k=1}^K \\left[ \\mathbf{E}[z_{nk}][log\\pi_k - \\frac{1}{2}log(2\\pi) -\\frac{1}{2}log(\\sigma_k^2) -\\frac{(x_n-\\mu_k)^{T}(x_n-\\mu_k)}{2 \\sigma_k^2}] \\right] \\\\\n",
    "=const - \\frac{1}{2} \\sum_{n=1}^N \\sum_{k=1}^K \\mathbf{E}[z_{nk}] log(\\sigma_k^2) - \\sum_{n=1}^N \\sum_{k=1}^K \\mathbf{E}[z_{nk}] \\frac{(x_n-\\mu_k)^{T}(x_n-\\mu_k)}{2 \\sigma_k^2}\n",
    "$$\n",
    "Note that we have ignored constant terms here. Specially in general GMM we had term $ \\mathbf{E}[z_{nk}]log\\pi_k$ there but we no loger have it because $\\pi_k$ is known to us and thus it also becomes a constant. Taking derivatives and setting to 0, we get For any class $k$,\n",
    "$$\n",
    "\\frac{\\partial(ECLL)}{\\partial(\\mu_k)}=\\sum_{n=1}^N \\frac{(x_n-\\mu_k)\\mathbf{E}[z_{nk}]}{\\sigma_k^2}\n",
    "$$\n",
    "Set above to zero and we get:\n",
    "$$\n",
    "\\hat{\\mu_k}=\\frac{\\sum_{n=1}^N \\mathbf{E}[z_{nk}] x_n }{\\sum_{n=1}^N \\mathbf{E}[z_{nk}]}\n",
    "$$\n",
    "When we do second derivative for it we clearly have $\\frac{\\partial^2(ECLL)}{\\partial(\\mu_k)^2}<0$. So $\\hat{\\mu_k}$ also passes second order maxima condition. Now for $\\sigma_k^2$ we do the same,\n",
    "$$\n",
    "\\frac{\\partial(ECLL)}{\\partial(\\sigma_k^2)}=\\sum_{n=1}^N \\mathbf{E}[z_{nk}] \\frac{(x_n-\\mu_k)^{T}(x_n-\\mu_k)}{2 \\sigma_k^4} -\\frac{1}{2} \\sum_{n=1}^N\\frac{\\mathbf{E}[z_{nk}]}{\\sigma_k^2}\n",
    "$$\n",
    "Set above to zero and we get,\n",
    "$$\n",
    "\\hat{\\sigma_k^2}=\\frac{\\sum_{n=1}^N \\mathbf{E}[z_{nk}] (x_n-\\hat{\\mu_k})^{T}(x_n-\\hat{\\mu_k})}{\\sum_{n=1}^N \\mathbf{E}[z_{nk}]}\n",
    "$$\n",
    "For $\\sigma_k^2$ also the second order maxima condition is satisfied. AS we know $\\pi_k$, we don't need to get its estimate. This completes the M-step for EM algorithm. After maximizing we get these results, here we have to compute $\\mathbf{E}[z_{nk}]$. Estimate value for it is as follows,\n",
    "$$\n",
    "\\mathbf{E}[z_{nk}]=0 x p(z_{nk}=0|x_n,\\Theta) + 1 x p(z_{nk}=1|x_n,\\Theta) \\\\\n",
    "=p(z_{nk}=1|x_n,\\Theta)\\\\\n",
    "=\\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k,\\sigma_k^2 I)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n|\\mu_j,\\sigma_j^2 I)}; \\forall n,k\n",
    "=\\gamma_{nk}\n",
    "$$\n",
    "\n",
    "Now we write EM algo for a general iteration $t$, just keep in mind that to update for $(t)$ we use values from previous iteration $(t-1)$.\n",
    "\n",
    "**EM Iterations:**\n",
    "---\n",
    "1. Initalize $\\Theta=\\{\\mu_k,\\sigma_k^2\\}_{k=1}^K$ as $\\Theta^{(0)}, and set $t=1$\n",
    "2. Repeat until convergence,\n",
    "    - **E-step:** compute expectation($\\gamma_{nk}$) of each $z_n$ as they are needed in M-step updates. Here $\\sigma_k^2 I$ is replaced by $\\Sigma_k$ for compact notation.\n",
    "    $$\n",
    "    \\mathbf{E}[z_{nk}^{(t)}]=\\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k^{(t-1)},\\Sigma_k^{(t-1)})}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n|\\mu_k^{(t-1)},\\Sigma_k^{(t-1)} }; \\forall n,k\n",
    "    $$\n",
    "    - **M-step:** With $\\gamma_{nk}$ in our hand we can re-estimate $\\Theta$ via MLE solutions that we got in maximization. Note that in updates $\\sum_{n=1}^N \\gamma_{nk} = N_k$. $N_k$ is the effective number of points in the k_th cluster. For {\\mu,\\sigma^2} updates in each k:\n",
    "\n",
    "    $$\n",
    "    \\mu_k^{(t)}=\\frac{1}{N_k}\\sum_{n=1}^N \\mathbf{E}[z_{nk}^{(t)}]=\\frac{1}{N_k}\\sum_{n=1}^N \\gamma_{nk}^{(t)}\n",
    "    $$\n",
    "    $$\n",
    "    \\hat{\\sigma_k^2}^{(t)}=\\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk}^{(t)} (x_n-\\mu_k^{(t)})^{T}(x_n-\\mu_k^{(t)})\n",
    "    $$\n",
    "3. For next iteration set $t=t+1$ and repeat E-M steps until convergence critarian is met "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implement this algorithm and show that it works on synthetic data with 3 clusters. If you are unable to derive the EM algorithm for part (a), implement the EM algorithm I showed in the class slides for the standard GMM (5 point penalty for taking this option) [15 points]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have taken 2D Gaussians so that we can visualize the results. Though algo works for any dim Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(mus,sigmas,n_points=200):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(A,X,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0]), array([0, 0]), array([0, 0]), 2, 2, 2]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n",
      "[array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([3.80367968, 3.01288329]), array([9.5428177]), array([9.5428177]), array([9.5428177])]\n"
     ]
    }
   ],
   "source": [
    "data_dim=2\n",
    "n_syn_cluster=3\n",
    "n_points=200\n",
    "class_probs=[0.3,0.3,0.4]\n",
    "n_samples=[60,60,80] # automate\n",
    "values=list(range(0,n_syn_cluster))\n",
    "true_theta=[np.array([1.4, 1.6]), np.array([2.4, 5.4]), np.array([6.4, 2.4]),np.array([1]),np.array([1]),np.array([1])]\n",
    "syn_data=[]\n",
    "for i in range(n_syn_cluster):\n",
    "    syn_data+=list(np.random.multivariate_normal(true_theta[i], true_theta[i+n_syn_cluster]*np.eye(data_dim), n_samples[i]))\n",
    "syn_data=np.array(syn_data)\n",
    "np.random.shuffle(X) \n",
    "## above is only one of the way to generate data; others are also there\n",
    "\n",
    "init_theta=[np.array([0, 0]), np.array([0, 0]), np.array([0, 0]),2,2,2] # means stored in array\n",
    "curr_theta=init_theta\n",
    "diff=100\n",
    "error_tol=1e-10\n",
    "iter_count=0\n",
    "max_iter=100\n",
    "hist_theta=[]\n",
    "hist_theta.append(curr_theta)\n",
    "\n",
    "while (diff>error_tol) and (iter_count<max_iter):\n",
    "    print(curr_theta)\n",
    "    iter_count+=1\n",
    "\n",
    "    # E-step\n",
    "    Ep=np.empty((n_points,n_syn_cluster))\n",
    "    for i in range(n_syn_cluster):\n",
    "        Ep[:,i]=class_probs[i]*multivariate_normal.pdf(x=syn_data,mean=curr_theta[i],cov=curr_theta[n_syn_cluster+i]*np.eye(data_dim))\n",
    "    Ep=Ep/Ep.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # M-step\n",
    "    tmp_theta=list()\n",
    "    for i in range(n_syn_cluster):\n",
    "        tmp_theta.append((np.dot(syn_data.T,Ep[:,i].reshape((n_points,1)))/np.sum(Ep[:,i])).reshape((1,data_dim))[0])\n",
    "    for i in range(n_syn_cluster):\n",
    "        tmp_theta.append(np.dot(np.sum(np.multiply((syn_data-tmp_theta[i]),(syn_data-tmp_theta[i])),axis=1).T,Ep[:,i].reshape((n_points,1)))/np.sum(Ep[:,i]))\n",
    "\n",
    "    # convergence criterian update\n",
    "    # using abs diff :: could use other techniques\n",
    "    diff=np.max(np.abs(np.concatenate(tmp_theta,axis=None)-np.concatenate(curr_theta,axis=None)))\n",
    "\n",
    "    # print(tmp_theta)\n",
    "    # next iter update\n",
    "    curr_theta=tmp_theta\n",
    "    hist_theta.append(tmp_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[3.57596954, 3.00066189]])[0].reshape(1,2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data-tmp_theta[i].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply((syn_data-tmp_theta[i]),(syn_data-tmp_theta[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
