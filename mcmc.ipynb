{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "#### Let’s consider a simple demonstration of MCMC sampling in a setting where conjugacy is actually possible – normal likelihoods with a known population variance, for which the prior is another normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part(a)\n",
    "\n",
    "#### Write a function to calculate the Bayesian posterior probability given 50 new data samples drawn from a normal distribution with mean 10 and SD 5, assuming a normal prior with mean 25 and s.d. 5. Plot the pdfs of the prior, the likelihood and the posterior distributions. Explain how you derive the likelihood from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this takes likelihood and prior parameters as input\n",
    "and returns posterior mean and sd using the formulas \n",
    "their derivation is explained in markdown comments\n",
    "'''\n",
    "def posterior(n_samples=50,lik_mean=10,lik_sd=5,prior_mean=25,prior_sd=5):\n",
    "    # to reproduce random state is fixed\n",
    "    np.random.seed(1)\n",
    "    samples=np.random.normal(lik_mean,lik_sd,n_samples)\n",
    "   \n",
    "    # formulas derivations for these calc explained in markdowns\n",
    "\n",
    "    post_sd=np.sqrt((lik_sd**2*prior_sd**2)/(lik_sd**2+prior_sd**2))\n",
    "\n",
    "    post_mean=((lik_sd**2)*(prior_mean)+(n_samples*prior_sd**2)*(np.mean(samples)))/((n_samples*prior_sd**2)+lik_sd**2)\n",
    "\n",
    "    return {'post_mean':round(post_mean,2),'post_sd':round(post_sd,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes mean and sd of an normal dist\n",
    "and returns \n",
    "    - a equi space points around mean with 3*sd width and \n",
    "    - its pdf value for given normal dist in a dictionary \n",
    "'''\n",
    "def norm_data(mean,sd):\n",
    "    x=np.linspace(-3*sd+mean,mean+3*sd,100)\n",
    "    norm_pdf = (1/(np.sqrt(2*np.pi) * sd )) * np.exp(-0.5*((x-mean)/sd)**2)\n",
    "    return {'x':list(x),'pdf':list(norm_pdf)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To plot lik-prior-posterior data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init given info\n",
    "n_samples=200\n",
    "lik_mean=10\n",
    "lik_sd=5\n",
    "prior_mean=25\n",
    "prior_sd=5\n",
    "\n",
    "# call posterior function to get params values with default values\n",
    "post=posterior()\n",
    "\n",
    "# get x-y data for plots\n",
    "lik_data=norm_data(lik_mean,lik_sd)\n",
    "prior_data=norm_data(prior_mean,prior_sd)\n",
    "post_data=norm_data(post['post_mean'],post['post_sd'])\n",
    "\n",
    "# plot pdf compare graph\n",
    "plt.plot(lik_data['x'],lik_data['pdf'],label='lik',color='black')\n",
    "plt.plot(prior_data['x'],prior_data['pdf'],label='prior',color='red')\n",
    "plt.plot(post_data['x'],post_data['pdf'],label='posterior',color='green')\n",
    "plt.legend()\n",
    "plt.title(\"Density plot for lik, prior, and posterior\")\n",
    "plt.ylabel('pdf-values')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observations:\n",
    "- for these specific values\n",
    "    - posterior mean has shifted in the direction of prior only by a very small amount(0.17) and \n",
    "    - sd has dropped to 3.536 from 5 that was for likelihood and prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Likelihood Derivation from Data\n",
    "\n",
    "We basically have to derive the likehood from the samples we generated in `posterior` function. We will consider a general situation with mean $\\mu$ and sd $\\sigma$. Our data is just a specific case of that.\n",
    "\n",
    "Now, consider N i.i.d. scalar obs $X = \\{x_1 , x_2 , \\ldots , x_N \\}$ drawn from a normal distribution with mean $\\mu$ and sd $\\sigma$. So, $\\forall x \\in \\{1,\\ldots,N\\}$ we have :\n",
    "$$\n",
    "p(x_n|\\mu,\\sigma^2)=\\mathcal{N}(x_n|\\mu,\\sigma^2)\n",
    "$$\n",
    "\n",
    "Or we can write,\n",
    "$$\n",
    "p(x_n|\\mu,\\sigma^2)  = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp\\left[-\\frac{(x_n-\\mu)^2}{2\\sigma^2}\\right]\n",
    "$$\n",
    "\n",
    "Using above we can give joint likelihood simply as their product(as they are iid),\n",
    "$$\n",
    "p(X|\\mu,\\sigma^2)  = \\prod_{n=1}^N p(x_n|\\mu,\\sigma^2)\n",
    "$$\n",
    "\n",
    "Full expanded version can be given as,\n",
    "$$\n",
    "p(X|\\mu,\\sigma^2)  = \\frac{1}{(2\\pi)^{n/2} \\sigma^n}exp\\left[-\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n-\\mu)^2\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### posterior derivation formulas explained\n",
    "\n",
    "To get posterior for `mean`($\\mu$) from lik and prior we can use bayes formula,\n",
    "$$\n",
    "p(\\mu|X)=\\frac{p(X|\\mu)p(\\mu)}{p(X)}=\\frac{p(X|\\mu)p(\\mu)}{\\int p(X|\\mu)p(\\mu)}\n",
    "$$\n",
    "\n",
    "Up to proportinality we write(as integral is just a constant ),\n",
    "$$\n",
    "p(\\mu|X) \\propto p(X|\\mu)p(\\mu)\n",
    "$$\n",
    "\n",
    "**`Lik:`**\n",
    "\n",
    "Given in above section. We will ignore constants(w.r.t. to $\\mu$) though.\n",
    "\n",
    "**`Prior:`**\n",
    "\n",
    "For general case when prior for $\\mu$ is a Guassian with mean $\\mu_0$ and sd $\\sigma_0$, we can write prior distribution,\n",
    "$$\n",
    "p(\\mu|\\mu_0,\\sigma_0^2)  = \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}}exp\\left[-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right]\n",
    "$$\n",
    "\n",
    "**`Posterior:`**\n",
    "\n",
    "Use Baues rule upto constant,\n",
    "$$\n",
    "p(\\mu|X) \\propto \\left(\n",
    "    \\frac{1}{(2\\pi)^{n/2} \\sigma^n}exp\\left[-\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n-\\mu)^2\\right]\\right) \\left(\\frac{1}{\\sqrt{2\\pi \\sigma_0^2}}exp\\left[-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right]\\right)\n",
    "$$\n",
    "\n",
    "WE also can ignore other constants like $\\sigma$(as it si known) and $\\sigma_0$,\n",
    "$$\n",
    "p(\\mu|X) \\propto \\left(\n",
    "    exp\\left[-\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n-\\mu)^2\\right]\\right) \\left(exp\\left[-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right]\\right)\n",
    "$$\n",
    "\n",
    "After using square trick and comaring with\n",
    "$$\n",
    "p(\\mu|X)  \\propto exp\\left[-\\frac{(\\mu-\\mu_N)^2}{2\\sigma_N^2}\\right]\n",
    "$$\n",
    "\n",
    "we get Gaussian posterior’s precision as the sum of the prior’s precision and sum of the noise\n",
    "precisions of all the observations,\n",
    "$$\n",
    "\\frac{1}{\\sigma_N^2}=\\frac{1}{\\sigma_0^z} + \\frac{N}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "and Gaussian posterior’s mean is a convex combination of prior’s mean and the MLE solution,\n",
    "$$\n",
    "\\mu_N=\\frac{\\mu_0 \\sigma^2+\\bar{x}N\\sigma_0^2}{N\\sigma_0^2+\\sigma^2}\n",
    "$$\n",
    "\n",
    "Here: $\\bar{x}=\\frac{\\sum_{n=1}^N x_n}{N}$\n",
    "\n",
    "NOTE: these formulas have been used in `posterior()` function to get posterior mean and SD parameter values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part(b)\n",
    "\n",
    "#### Implement the Metropolis algorithm from the lecture slides to estimate the posterior distribution given the same prior and data and show that it converges to the analytic posterior by plotting a histogram of samples from the distribution alongside the analytic posterior distribution.  Assume whatever SD (width) you want for the proposal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Derivation\n",
    "\n",
    "Now suppose that we did not know about square trick then it won't have been easy to do this integral and compute above posterior in closed and nice form. And that happens all the time in real worl dproblem. So, we are going to experiment with this problem where we suppose we don't know how to solve above problem and we will use Monte Carlo Markov Chains to get enough samples from the posterior distribution formula up to prop constant and we call this our `Target` distribution from which we need samples but we can not as it is not a standard known dist. That is why we use a proposal dist(TBD) that conditions on the immediately previous $\\mu$ value (say a Gaussian).\n",
    "\n",
    "Here we have to decide what to use as our proposal(jump) distribution. Since we know our posterior(target dist) has support all over the real line, we can consider Univariate Norrmal Distribution for this case as it does cover real line and also simple sample from. \n",
    "\n",
    "So our proposal distribution is a Normal Distribution. \n",
    "\n",
    "**`Algo Steps:`** A general MH algo steps are,\n",
    "\n",
    "Let $x_0 =init$ and for a $k^{th}$ step when $x_k=x$ and to obtain $x_{k+1}$ we do these steps:\n",
    "1. Sample a new possible proposal $y$ from the jump distribution that conditions on the immediately previous (that is $x$ here) value.\n",
    "2. Calculate the ratio of the proposed posterior distribution to the current one. The ratios is given by\n",
    "$$\n",
    "r=\\frac{f(y)g(x|y)}{f(x)g(y|x)}\n",
    "$$\n",
    "\n",
    "But since we are using guassian dist as our proposal we have $g(x|y)=g(y|x)$. meaning it is symmatric. We have same probability of going from x to y as y to x. So, remember here f is our posterior distribution upto prop constant,\n",
    "$$\n",
    "r=\\frac{f(y)}{f(x)}\n",
    "$$\n",
    "\n",
    "3. Sample a random number a between 0 and 1 using `Uniform dist` --> $U(0,1)$. Say the sample is `a`\n",
    "4. If r > a, accept the proposed $y$, otherwise stick with the earlier $y$\n",
    "5. This gives one sample from the target distribution. We can repeat above steps to get more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to compute target dist's probability value in each run\n",
    "takes a point from support space(real line here)\n",
    "returns log of the posterior dist at that point\n",
    "we have used log for computational stability(it does not affect inequality condition for choosing samples from MCMC as it is monotionic)\n",
    "'''\n",
    "def log_post(x,mean,sd):\n",
    "    # log og posterior up to prop constant\n",
    "    log_p=-((x-mean)/sd)**2\n",
    "    \n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes hyperparameter values and\n",
    "returns mcmc samples\n",
    "other things explained in func comments\n",
    "'''\n",
    "no_of_samples=10000\n",
    "def mh(jump_sd=5,init=10,no_of_samples=no_of_samples):\n",
    "    global post\n",
    "    global \n",
    "    # to reproduce results\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # list to store samples\n",
    "    mcmc_samples=list()\n",
    "\n",
    "    x_curr=init\n",
    "    '''\n",
    "    x_curr=10 # inital value --> hyper-para\n",
    "    no_of_samples=10000 # hyper-para\n",
    "    jump_sd=5 # hyper-para\n",
    "    '''\n",
    "\n",
    "    acc_count=0\n",
    "    for i in range(no_of_samples):\n",
    "        # propose a new value from jump dist\n",
    "        y_prop=np.random.normal(x_curr,jump_sd)\n",
    "\n",
    "        # calc ratio( in log ) --> posterior parameters\n",
    "        r=(log_post(y_prop,post['post_mean'],post['post_sd']))-(log_post(x_curr,post['post_mean'],post['post_sd']))\n",
    "\n",
    "        # propose random sample from uniform\n",
    "        u=np.random.uniform(0,1)\n",
    "\n",
    "        # accept-reject condition for MH\n",
    "        if r > np.log(u):\n",
    "            mcmc_samples.append(y_prop)\n",
    "            # update value of current sample\n",
    "            x_curr=y_prop\n",
    "            # update acceptance count\n",
    "            acc_count+=1\n",
    "        else:\n",
    "            mcmc_samples.append(x_curr)\n",
    "    \n",
    "    print(f'acceptance rate is {acc_count*100/no_of_samples} with jump SD={jump_sd}')\n",
    "    return mcmc_samples\n",
    "\n",
    "\n",
    "# actual posterior params that we got in first que is used\n",
    "actual_samples=np.random.normal(post['post_mean'],post['post_sd'],no_of_samples)\n",
    "# func return\n",
    "mcmc_samples=mh()\n",
    "# plot hists to compare\n",
    "plt.hist(mcmc_samples,label='estimate(MCMC)',bins=100,alpha=0.3,density=True)\n",
    "plt.hist(actual_samples,label='analytic',bins=100,alpha=0.3,density=True)\n",
    "plt.legend()\n",
    "plt.title(\"hist plot for mcmc and posterior(actual) samples\")\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the speed of convergence of the sampling depend on the proposal width? Is there an optimal proposal width that would work best? Demonstrate the consequences of using sub-optimal proposal width and terminating sampling too soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- todos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1000,  -999,  -998, ...,   998,   999,  1000]),\n",
       " array([0.84942738, 0.84965309, 0.85049724, ..., 0.85049724, 0.84965309,\n",
       "        0.84942738]),\n",
       " <matplotlib.collections.LineCollection at 0x7efe25be4970>,\n",
       " <matplotlib.lines.Line2D at 0x7efe2674df70>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.acorr(mcmc_samples,maxlags=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "From the above example it is clear that the quality of the MCMC sampler is determined by the choice of the variance in the proposal distribution(SD).\n",
    "- If SD is too large, then we will propose values potentially too far away, which in principle would be good, but this means a lot of the values will be rejected. That\n",
    "means we will stay where we are quite often, increasing the correlation in the sample.\n",
    "- If SD is too small, then we will make tiny moves implying we may accept a lot of them, but the samples obtained will be quite dependent.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "365d70965140afb04a698773bfdd31483bc82432b779112c2a78b5de7c16d125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
